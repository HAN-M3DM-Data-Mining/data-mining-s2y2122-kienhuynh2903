---
title: "Assigment - Naive Bayes"
author:
  - Kien Huynh Trung - Author
  - Dang Nguyen - Reviewer
date: March 20, 2022
output:
   html_notebook:
    toc: true
    toc_depth: 2
---

```{r}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
library(dplyr)
```
---
NB-fakenews.csv

## Business Understanding
Identify fake news using Naive Bayes model. 

## Data Understanding
```{r}
url <- "https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/data-mining-s2y2122-kienhuynh2903/master/datasets/NB-fakenews.csv"
rawDF <- read.csv (url)
rawDF<- rawDF[-c(1:15000),]
rawDF$id <- NULL 
rawDF$title <- NULL
rawDF$author <- NULL
rawDF <- rawDF %>% relocate(label, .before = text)
rawDF <- mutate(rawDF, label = recode(label,"0"= "ham", "1"= "spam"))
head (rawDF)
rawDF$label <- rawDF$label %>% factor %>% relevel("spam")
class(rawDF$label)

spam <- rawDF %>% filter(label == "spam")
ham <- rawDF %>% filter(label == "ham")

wordcloud (spam$text, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud (ham$text, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))
gc()
```

## Data Preparation
```{r}
rawCorpus <- Corpus(VectorSource(rawDF$text))
inspect(rawCorpus[1:3])
cleanCorpus <- rawCorpus %>% tm_map(tolower) %>% tm_map(removeNumbers)
cleanCorpus <- cleanCorpus %>% tm_map(tolower) %>% tm_map(removeWords, stopwords()) %>% tm_map(removePunctuation)
cleanCorpus <- cleanCorpus %>% tm_map(stripWhitespace)
cleanCorpus <- tm_map(cleanCorpus, removeWords, stopwords("english"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("the"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("we"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("also"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("will"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("one"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("said"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("just"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("even"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("a"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("an"))
cleanCorpus <- cleanCorpus %>% tm_map(removeWords,("like"))

wordcloud (spam$text, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud (ham$text, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))
gc()

tibble(Raw = rawCorpus$content[1:3], Clean = cleanCorpus$content[1:3])

cleanDTM <- cleanCorpus %>% DocumentTermMatrix
set.seed(1234)
trainIndex <- createDataPartition(rawDF$label, p = .75, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)

trainDF <- rawDF[trainIndex, ]
testDF <- rawDF[-trainIndex, ]

trainCorpus <- cleanCorpus[trainIndex]
testCorpus <- cleanCorpus[-trainIndex]

trainDTM <- cleanDTM[trainIndex, ]
testDTM <- cleanDTM[-trainIndex, ]

freqWords <- trainDTM %>% findFreqTerms(500)
trainDTM <-  DocumentTermMatrix(trainCorpus, list(dictionary = freqWords))
testDTM <-  DocumentTermMatrix(testCorpus, list(dictionary = freqWords))

convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("No", "Yes"))
}

nColsDTM <- dim(trainDTM)[2]
trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)

head(trainDTM[,1:10])

```
## Modeling
```{r}
nbayesModel <-  naiveBayes(trainDTM, trainDF$label, laplace = 1)
predVec <- predict(nbayesModel, testDTM)
confusionMatrix(predVec, testDF$label, positive = "spam", dnn = c("Prediction", "True"))

```
## Evaluation and Deployment
The model gives the result of 72.8 percent of accuracy level. I tried to remove unneccessary words but it seems not to be helpful. 

Hence, the reviewer may add suggestion to improve the model. 

## Final Result
Confusion Matrix and Statistics

          True
Prediction spam  ham
      spam 2106  913
      ham   497 1683
                                          
               Accuracy : 0.7288          
                 95% CI : (0.7165, 0.7408)
    No Information Rate : 0.5007          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.4575          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.8091          
            Specificity : 0.6483          
         Pos Pred Value : 0.6976          
         Neg Pred Value : 0.7720          
             Prevalence : 0.5007          
         Detection Rate : 0.4051          
   Detection Prevalence : 0.5807          
      Balanced Accuracy : 0.7287          
                                          
       'Positive' Class : spam            
                                          
